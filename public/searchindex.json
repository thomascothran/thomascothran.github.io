{"categories":[{"title":"architecture","uri":"https://thomascothran.tech/categories/architecture/"},{"title":"clojure","uri":"https://thomascothran.tech/categories/clojure/"},{"title":"CS","uri":"https://thomascothran.tech/categories/cs/"},{"title":"design","uri":"https://thomascothran.tech/categories/design/"},{"title":"Evidence-Based-Software","uri":"https://thomascothran.tech/categories/evidence-based-software/"}],"posts":[{"content":"In the Pragmatic Programmer, Andy Hunt and Dave Thomas tell us: “it’s critical that you write code that is readable and easy to reason about.” This seems uncontroversial; it is the rare point on which software engineers typically agree. Or do they?\nIn fact, developers disagree about what “readability” means. \u0026ldquo;Readability\u0026rdquo; can be given two contrary meanings that we will call readabilitya and readabilityi.\nReadabilitya controls complexity by the use of abstractions. Readabilityi, on the other hand, is characterized by clearly expressing the implementation.\nWhy does it matter? If we think in terms of readabilityi\nOur Clojure code will look a lot like Java 7 with parenthesis. We won\u0026rsquo;t functional programming techniques like function composition and higher order functions, nor will we make use of polymorphism. Rather, our namespaces will be essentially be classes and our functions static methods. They will be combined in imperative fashion. Spaghetti code will be the norm. For example, our business rules and our use cases will tend to be tied to our data sources. Our data layer will be at the center of the application, since we lack the facilities to abstract and invert the dependencies. Our codebase will not be expressive of the domain. Code that is readablei requires developers to understand the domain independently of the code base. Code that is readablea teaches you the domain. An Example To spell out the difference, we will use an example based approach. Our hypothetical application is a financial services application that, among other things, handles loans and loan applications.\nIt uses a microservices architecture. Our first example will concern calls between services.\n(ns finco.loans (:require [org.httpkit.client :as http] [next.jdbc.sql :as sql] [taoensso.timbre :as timbre] [jsonista.core :as j] [jackdaw.client :as kafka-client]) (:import [java.time.temporal ChronoUnit]) (defn approve-loan [{:keys [auth-token] :as config} {:keys [jdbc-conn kafka-producer] :as connections} {:keys [state first-name last-name date-of-birth debt income] :as applicant {:keys [trace-id username user-id] :as ring-request}] (let [state-minimum-age (-\u0026gt; (sql/find-by-keys jdbc-conn :state_minimum_loan_ages {:state state}) first :state_minimum_loan_ages/minimum_age) years-old (.between ChronoUnit/YEARS date-of-birth (LocalDate/now)) old-enough? (\u0026lt;= state-minimum-age years-old) _ (timbre/info \u0026quot;Requesting credit score for \u0026quot; first-name \u0026quot; \u0026quot; last-name \u0026quot;. \u0026quot; trace-id) credit-score (-\u0026gt; @(http/get \u0026quot;https://fincoloans.tech/api-gateway/credit-scores\u0026quot; {:query-params {:first-name first-name :last-name last-name :dob date-of-birth :headers {\u0026quot;Authentication\u0026quot; (str \u0026quot;Bearer \u0026quot; token)} \u0026quot;Trace-ID\u0026quot; trace-id \u0026quot;Username\u0026quot; username \u0026quot;UserID\u0026quot; user-id}}}) :body j/read-value) sufficient-credit? (\u0026lt; 650 credit-score)] (if (and old-enough? sufficient-credit?) (do (timbre/info \u0026quot;Loan approved \u0026quot; applicant \u0026quot; - \u0026quot; trace-id) (sql/insert! jdbc-conn :loan_approvals (merge application loan)) (kafka-client/produce! kafka-producer \u0026quot;loan-approvals\u0026quot; (j/write-value-as-string (merge application loan))}) :approved) (do (timbre/info \u0026quot;Loan Denied \u0026quot; applicant \u0026quot; - \u0026quot; trace-id) (kafka-client/produce kafka-producer \u0026quot;loan-denials\u0026quot; (j/write-value-as-string (merge application loan)) :denied)) We are not handling errors, and the actual business logic of a use case like this is more complex, but you get the idea.\nThis function is readablei, but not readablea.\nReadabilityi It is readablei because it clearly expresses the details of how it works. We can learn quite a bit by reading the function:\nWhat the data sources are. We use http to request the credit score from one of our other microservices, rather than fetching it from a database or using an external service. We use a database table for the minimum age for a state. How authentication is done handled between services. We use a bearer token and add the username and user id to track just who is requesting the credit score. How tracing is implemented. We are using timbre and adding the trace in the logs. What libraries we use: http-kit, timbre, and jsonista. Readabilityi is an answer to the question \u0026ldquo;how does this work?\u0026rdquo;. All the implementation details are \u0026ldquo;out front\u0026rdquo;. One reads them and infers what the program is for.\nNow, clearly our approve-loan function is poorly written. Functions should \u0026ldquo;do one thing\u0026rdquo;, but our function does a number of things: calls various services, handles cross-cutting concerns like logging and tracing, implements authentication, updates the database and Kafka (in a non-transactional way).\nWhat\u0026rsquo;s important to note is the reason approve-loan is poorly written: precisely because it is readablei. This means that if we expect functions in our codebase, especially the high level use cases, to clearly express how they work, then we are the problem.\nTo repeat: the insistence on code being readable (in the sense of expressing how it works, preferring concrete implementations to abstractions) produces bad software. Readablei is easy, not simple.\nAbstraction By contrast, readabilitya is readable because of its abstractions. Abstraction is not a programming-specific idea. It is best illustrated by scientific discovery. In scientific discovery, the inessentials are set aside, and a mental construction is used to unite things under a single idea.\nFor example, Newton noticed that apples always fall perpendicular to the ground. His insight was into the idea that all things with mass might obey the same law of attraction: whether they be apples, planets, or tides. In formulating his law of gravitation, Newton was able to set aside the inessential (whether a thing is red or yellow, small or large, moving or still); what matters is simply a thing\u0026rsquo;s mass.\nBut the real moment of abstraction is not subtractive, it is creative. It is the production of a mental construct that brings order. In Newton\u0026rsquo;s case, this is the law of universal gravitation: the force of gravity is proportional to the product of the mass of two objects divided by distance between their centers.\nThe scientific example is not merely illustrative. The ability to abstract is what makes human beings intelligent. Recognizing patterns and repeating them to \u0026ldquo;get things working\u0026rdquo; are capacities we share with animals. But the construction of meaning \u0026ndash; of theorems, explanations, and \u0026ndash; yes \u0026ndash; functions in Clojure \u0026ndash; belongs to higher, human intelligence.\nReadabilitya In the context of programming, the subtractive is important but secondary to the constructive moment. Nevertheless, the question \u0026ldquo;what is inessential\u0026rdquo; is the best place to start.\nWhat do we not need to know about approve-loan to accomplish our purpose?\nHow requests are authorized. This can be shuttled off to a function. How logging is implemented. This is a cross-cutting concern. Whether we use a database, a microservice we own, or an external microservice as a data source. Let\u0026rsquo;s take a ports and adapters approach.\n(ns finco.loans) (defmulti get-credit-score (fn [data-source {:keys [state first-name last-name date-of-birth] :as applicant}] (class data-source))) (defmulti state-\u0026gt;minimum-age (fn [data-source state] (class data-source) (defmulti create-approval! (fn [data-source applicant loan] (class data-source)) (defn approve-loan2 [data-sources {:keys [state first-name last-name date-of-birth debt income] :as applicant {:keys [amount] :as loan} {:keys [on-success! on-denial!]}] (let [years-old (.between ChronoUnit/YEARS date-of-birth (LocalDate/now)) old-enough? (\u0026lt;= (state-\u0026gt;minimum-age (:state-laws data-sources) state) years-old) sufficient-credit? (\u0026lt; 650 (get-credit-score (:credit-scores data-sources) applicant))] (if (and old-enough? sufficient-credit?) (do (create-approval! (:applications data-sources) applicant loan) (when on-success! (on-success! applicant-loan) :approved) (do (when on-denial! (on-denial! applicant loan)) :denied) Notice how simple this is compared to our previous implementation. Previously we were coupled to everything from a particular http library to the format of the responses from our services. Now we can focus on the application logic.\nOur application logic is quite stable. We could swap out the source for where we get our credit scores, or we could split out further microservices. If we decide to add a cache, we could use on-success! to populate it.\nIn removing the inessentials, the expression of the essentials becomes much more clear. A developer who knows nothing about loans could get a good idea of what the business logic is just by glancing at this use case.\nSimple / Easy A developer who cannot think abstractly may have trouble with approve-loan2. \u0026ldquo;Why can\u0026rsquo;t I jump to the source?\u0026rdquo; \u0026ldquo;What is this really doing?\u0026rdquo; But these are the wrong kinds of questions to be asking at the level of the business rules or the application logic.\nBut in practice, defenses of readabilityi usually come in in the practice of writing code. It is certainly the case that what is easier to write in the moment takes less work than forming the right abstractions, thinking through semantic layers, and so on.\nBut this is hard not because it is more typing, but because thinking through things intelligently, formulating ideas clearly and rigorously is hard work. It is easier to follow a pattern, easier to \u0026ldquo;get things working\u0026rdquo;, and delegate the understanding of a business to a product owner or business analyst.\nHere, the problem is not with abstractions. It is the limitation of one\u0026rsquo;s own abilities. Fortunately, good software engineers are driven to self-improvement, and the challenge here is one of intellectual development.\n","id":0,"section":"posts","summary":"In the Pragmatic Programmer, Andy Hunt and Dave Thomas tell us: “it’s critical that you write code that is readable and easy to reason about.” This seems uncontroversial; it is the rare point on which software engineers typically agree. Or do they?\nIn fact, developers disagree about what “readability” means. \u0026ldquo;Readability\u0026rdquo; can be given two contrary meanings that we will call readabilitya and readabilityi.\nReadabilitya controls complexity by the use of abstractions.","tags":["abstraction","clojure"],"title":"The Wrong Kind of Readability","uri":"https://thomascothran.tech/2023/11/readability/","year":"2023"},{"content":"The \u0026ldquo;Library Locker\u0026rdquo; is a common anti-pattern for incorporating third party libraries into an application. The application wraps the library with its own function, which mixes concerns and makes the library more difficult to use.\nIn this article, we will first consider the original problem the library locker is introduced to solve, then an example of the library locker, and then suggest better alternatives.\nThe Problem Suppose we have system of microservices that support astronomical applications. We locate our calls to our other microservices in a service directory.\nQuickly, we find a certain pattern repeating itself. We need to prepare a request map and parse the response.\n(ns app.service.night-sky \u0026quot;Handles interactions with Night-Sky, a microservice owned by our team\u0026quot; (:require [app.auth-tokens :as auth] [clj-http.client :as http] [clojure.tools.logging :as log] [jsonista :as j])) (defn find-star \u0026quot;Find a star in the night sky\u0026quot; [{user-id :user/id star-id :star/id root-trace :root-trace/id request-id :request/id :keys [originating-system] :or {originating-system \u0026quot;observatory\u0026quot; request-id (random-uuid)}}] (log/infof \u0026quot;User %s is requesting star %s. Request id: %s. Trace id %s\u0026quot; user-id request-id trace-id) (let [response (http/get \u0026quot;http://host.night-sky/api/star\u0026quot; {:oauth (:night-sky auth/tokens) :query-params {:star-id star-id} :headers {:user-id user-id :request-id request-id :root-trace root-trace :originating-system originating-system}}) body (j/read-value (:body response))] (log/infof \u0026quot;Received star: %s. Request id: %s. Trace id %s\u0026quot; body request-id trace-id) body)) Most of the body of find-star is not about finding a star. It\u0026rsquo;s about logging, tracing, and authentication. What if we wanted to change our logging library? Or improve our log messages?\nWe need an abstraction. But what should it look like?\nLibrary Locker Example We decide to pull out the common code in a single function called get. The purpose will be to make get calls to other services we own, while not needing to repeat ourselves in each service call.\n(ns app.http (:require [app.auth-tokens :as auth] [clj-http.client :as http] [clojure.tools.logging :as log] [jsonista :as j])) (defn get \u0026quot;Wrap the clj-http `get` call to unify handling of logging, tracing, and authentication. Parameters ---------- - `url`: the URL to call. - `http-options`: clj-http options\u0026quot; [url {:keys [query-params] :as http-options} {user-id :user/id star-id :star/id root-trace :root-trace/id request-id :request/id service :service/name :keys [originating-system] :or {originating-system \u0026quot;observatory\u0026quot;} :as options}] (log/infof \u0026quot;User %s is requesting star %s. Request id: %s. Trace id %s\u0026quot; user-id request-id trace-id) (let [response (http/get \u0026quot;http://host.night-sky/api/star\u0026quot; (-\u0026gt; http-options (assoc :oauth (get auth/tokens service)) (assoc-in [:headers :user-id] user-id) (assoc-in [:headers :request-id] request-id) (assoc-in [:headers :root-trace] root-trace) (assoc-in [:headers :originating-system] root-trace))) body (j/read-value (:body response))] (log/infof \u0026quot;Received star: %s. Request id: %s. Trace id %s\u0026quot; body request-id trace-id) body)) Now, we have a handy function, that looks similar to clj-http\u0026rsquo;s get function, except that it gives us some additional functionality.\nThis lets us dramatically simplify our find-star function:\n(ns app.service.night-sky \u0026quot;Handles interactions with Night-Sky, a microservice owned by our team\u0026quot; (:require [app.http :as http])) (defn find-star \u0026quot;What stars are in the night-sky?\u0026quot; [{user-id :user/id star-id :star/id root-trace :root-trace/id}] (http/get \u0026quot;http://host.night-sky/api/star\u0026quot; {:query-params {:star-id star-id}} {:user/id user-id :service/name :night-sky :root-trace/id root-trace})) Amazing! Or is it?\nNotice that we have reduced the functionality of clj-http\u0026rsquo;s library. clj-http.client/get has an async mode:\n(client/get \u0026quot;http://example.com\u0026quot; {:async? true} ;; respond callback (fn [response] (println \u0026quot;response is:\u0026quot; response)) ;; raise callback (fn [exception] (println \u0026quot;exception message is: \u0026quot; (.getMessage exception)))) But the async mode cannot be used. We\u0026rsquo;re locked into synchronous mode.\nMoreover, while it was convenient to parse the body of the response, we\u0026rsquo;ve reduced the amount of information we return. We only return the body. We couldn\u0026rsquo;t access the headers if we wanted to!\nIt would be very hard to write get using test driven development. And if it\u0026rsquo;s hard to test, it\u0026rsquo;s poorly designed.\nNotice that every function call within our app.http/get couples us to something:\nHttp library. We\u0026rsquo;re coupled to clj-http. If we were to want to use another library for a special purpose, like aleph, we cannot reuse our get function.\nLogging. While we might easily change the log library, note that we log the entire response. But some responses may be long, or may have information that cannot be logged for security reasons. What if we want to log some of the headers from a certain service? What we shouldn\u0026rsquo;t log a key in the response?\nParsing. What if we want to use transit instead of JSON for our responses? What if we want keywordized keys in some cases, but not other cases (where, say, there are keys with spaces in them)?\nAs the requirements around service calls grow, we will likely see a proliferation of parameters so that get can support a broader variety of behaviors. For example, if we want to excise sensitive information from the response body, we may pass a further parameter to get. Or, if we want to change the response format, we may pass in a :response/format argument.\nExtending the behavior of get will be error prone. We will be tempted to add defaults, such as a default timeout. This could easily break existing callers. Over time, the function signature will become more and more polluted, and get will be more complex to use and understand.\nWhat if we want to add validation on the parameters? Check the response against a contract? Stub out functions for testing?\nLibrary Locker Definition We can now offer something of a definition of the library locker antipattern. It occurs when:\nA third party library is wrapped in the application, such that: The functionality is reduced or changed, and The reduction or change is locked in the scope of the wrapper function, and The wrapper function typically extends its behavior by adding parameters Typically, this will result in a mixing of concerns, code churn on the wrapper function as new capabilities are needed, and the fragility that results from frequent modification.\nDesign Principles Functions should be focused. They should do one thing well. But our get function does many things: logging, authentication, parsing, etc.\nBut our goal is to have a function that we can use in our services namespaces to easily call our other services. Is there a better way?\nThe library locker is a case study in the opposite of the open-closed principle. We should be able to add functionality of our get functionality without changing get\u0026rsquo;s source code. Instead, our get function serves as a shell that prevents us from extending its functionality.\nAlternatives Let\u0026rsquo;s consider several alternatives.\nFunction Composition Clojure is a functional language. Consequently, function composition should be top of mind for us in designing flexible software. Without it, Clojure can quickly turn into a scripting language with parentheses.\nWe notice that we have three types of operations we\u0026rsquo;re doing in our get function:\nManipulation of the request map (e.g., adding headers) Handling the response (e.g., parsing the result) Doing something with both the request and the response (logging) We could easily compose functions:\n(defn trace \u0026quot;Trace a request\u0026quot; [trace-id req] (assoc-in req [:headers :trace-id] trace-id)) (defn identify-user \u0026quot;Identify the user to the service we are calling\u0026quot; [trace-id req] (assoc-in req [:headers :user-id] trace-id)) (defn identify-origin-system \u0026quot;Tell the callee service where the request originated\u0026quot; [req] (assoc-in req [:headers :origin-system] \u0026quot;observatory\u0026quot;)) (defn authenticate-service \u0026quot;Authenticate our service with the callee service\u0026quot; [service-name req] (assoc-in req [:headers :oauth] (get auth/tokens service-name))) (defn prepare-request [{user-id :user/id root-trace :root-trace/id request-id :request/id :keys [originating-system] :or {originating-system \u0026quot;observatory\u0026quot; request-id (random-uuid)}}] (comp (partial inject-root-trace root-trace) inject-origin-system inject-oath-token (partial inject-user-id user-id)) ) There are a few improvements that stand out immediately:\nComposability Extensibility Declarative abstraction Composability It could easily be the case that requests will have different needs. For example, not all requests may originate from an end user.\nBy separating behaviors into small, narrowly focused functions, and then composing them together, we can reuse each, and re-compose them in different circumstances.\nExtensibility Because we are composing functions, we always have the option of further function composition. Particular service namespaces, for instance, can compose prepare-request to their own unique needs.\nEach of our functions has become more declarative. Each of our functions says what it does, not how it does it. Instead of set-user-id-header, we identify-user. Should we have a different way of doing this in the future, we don\u0026rsquo;t need to change the function name.\nThis lets us not need to worry about how each function manipulates the request. Our function is more readable by hiding the details. Should anything change about how we identify our user in the future, we probably won\u0026rsquo;t need to change our prepare-request function.\nMoreover, this composition pattern would work with a number of functions in the same http client library, and by other http client libraries, should we choose to use them.\nMiddleware Function composition is simple, but has some limitations. Note that our functions can access only the request, not the request and response together. If this is a limitation, middleware might be a better pattern.\nclj-http supports middleware, but it is simple to roll your own in a library-independent fashion.\nAspect Oriented Programming Logging is a cross-cutting concern that can be handled with aspect-oriented approach. It can be handy to instrument functions to capture certain parameters and return values. AOP can be configured differently in different environments as well.\nFunctions can be made easier to read, because the logging is injected via a library like Robert Hooke or Hansel\nObjections There are two common responses to function composition-type patterns:\n\u0026ldquo;It\u0026rsquo;s less readable. I can\u0026rsquo;t see what it does! It\u0026rsquo;s more complicated, not less.\u0026rdquo; \u0026ldquo;Why spend time splitting things up, or establishing library-independent patterns? YAGNI.\u0026rdquo; These are, I would argue, misunderstandings of what we aim for with readability and YAGNI.\nReadability If function composition as such is less readable, my suggestion would be that the paradigm of functional programming has not been fully internalized. The trouble is not that function composition as such is less readable, but that a functional language like Clojure is less readable to those who don\u0026rsquo;t understand functional programming.\nBut the readability problem may go to an even more basic misunderstanding of good software design. Good software design separates \u0026ldquo;what\u0026rdquo; from \u0026ldquo;how\u0026rdquo;. It creates abstractions that allow you to use things by understanding what they are for, without needing to understand the internal mechanisms by which they accomplish it.\nFailure to establish this semantic separation means that one has to understand everything to do anything. It leads to situations where, in order to understand something as high level as domain logic, one has to worry about the structure of HTTP calls, database table structures, authorization, etc.\nIn other words, failure to establish semantic layers is the \u0026ldquo;easy\u0026rdquo; path that leads to the big ball of mud. And developers get used to thinking of readable code as an imperative sequence, where all the mechanisms are gathered together.\nThis is akin to the blub paradox. When one looks down at lower levels of abstraction, one appreciates the patterns with which one is familiar. But looking up to the levels above what one currently understands, patterns that enforce loose coupling and high cohesion, separation of concerns, etc., one thinks that it\u0026rsquo;s all \u0026ldquo;just more complex and harder to read.\u0026rdquo;\nThe solution is not to reduce code quality, but to elevate our own understanding of good software design, both in general software design and in functional programming.\nYAGNI Why spend the effort to make designs modular and extensible? Must we have a specific use case in mind to justify making code reusable?\nIf our software can only be used for the use cases we now about now, it will be both brittle and inept. It will be brittle, because we have to alter working code to support new functionality. It will be inept, in contrast to the power of composable, flexible software that is open to cases not initially anticipated.\n","id":1,"section":"posts","summary":"The \u0026ldquo;Library Locker\u0026rdquo; is a common anti-pattern for incorporating third party libraries into an application. The application wraps the library with its own function, which mixes concerns and makes the library more difficult to use.\nIn this article, we will first consider the original problem the library locker is introduced to solve, then an example of the library locker, and then suggest better alternatives.\nThe Problem Suppose we have system of microservices that support astronomical applications.","tags":["design","clojure"],"title":"The Library Locker - An Antipattern","uri":"https://thomascothran.tech/2023/08/library-locker/","year":"2023"},{"content":"This is the first in a multi-part series, \u0026ldquo;Brittle Clojure\u0026rdquo;. In this series, we will consider common patterns in Clojure which yield brittle systems, as well as methods to ensure robustness.\nNone of the basic principles for building robust software are unique. Most literature, however, is focused on object-oriented systems. Our point of view will sometimes zoom in to Clojure \u0026ldquo;in the small\u0026rdquo;, and sometimes zoom out to distributed systems built with Clojure.\nThroughout these articles, we will use as a hypothetical example a legal case management system, Atticus Case Management (ACM). Its users include attorneys, clerks, accountants, and clients. It supports a variety of use cases: from hours tracking, to billing, to document creation and review, to client communications. We will use ACM as an exaggerated example of anti-patterns.\nWe follow the narrative of Alice, a software developer, as she joins ACM.\nACM: A Distributed Clojure Application Alice is excited to join ACM and start working with Clojure. There are four teams of Clojure developers, and separate QA and product.\nThere is a single re-frame application on the front end, and about a dozen Clojure microservices. Postgres is the data store, with Kafka as a message bus.\nBen, her team lead, takes the time to walk her through some current features in flight. They will be working on enhancements to the billing system.\nAlice\u0026rsquo;s task is to ensure that a case cannot be closed if there is an outstanding, unpaid bill.\nHer story seems a little vague to her, so she wrote the following user story to clarify in her own mind what she was doing.\nIn order to ensure that clients pay their bill, as a secretary, I cannot close a case when there is an outstanding unpaid bill.\nAnd then she wrote:\nGiven that I am a secretary And a case has an outstanding, unpaid bill When I attempt to close the case Then I am prevented from doing so And I see a message telling me that there is an open invoice. There is a microservice called \u0026ldquo;billing\u0026rdquo;. Alice reads through the repository but struggles to understand it. She has no trouble identifying the handlers, database calls, but there is no single namespace that contains the business logic.\nAs Ben pair programs with her, she notices how easily he jumps around the code base. They start with a function that Alice was struggling to understand. Ben is able to jump through all the internal functions as though his Emacs were an extension of his mind.\nBen not only flits around the billing service code base, he opens the re-frame frontend to look at the event handlers. This is necessary to see how the billing API is being used. But, to understand the sequence of events in re-frame, he also needs to look at the \u0026ldquo;case\u0026rdquo; microservice.\nAlice stops Ben, asking him if there\u0026rsquo;s one place in the code base that encapsulates the business logic for her feature. \u0026ldquo;Is there one place I can go to answer the question, \u0026lsquo;who can do what, and under what conditions?\u0026rsquo;\u0026rsquo;\u0026rdquo;\nBen tells her that in order to understand the business rules, Alice will have to build up her mental model that encompasses the re-frame frontend, the billing service, and the case service.\nAlice tries again. She asks if there are tests she can look at that document the business logic. Ben tells her that, while their team does write unit tests for tricky functions, in general it is too hard to test a full workflow. Their services are interconnected, and much of the coordination is in re-frame. Where there\u0026rsquo;s a lot of IO, tests are not as useful, he tells Alice.\nBen continues that while he does agree that testing is important in general, on this project speed is more important than quality.\nREPL Driven Development? As they continue pairing, she discovers that Ben\u0026rsquo;s typical workflow involves clicking around in the front end application to fire the events to the backend. Even with his experience, he is not able to discern what the system does by looking at the code base. He has to manually imitate a user, and then look at the logs to observe what is happening.\nBen is quite good at using the REPL to capture and inspect values. However, Ben\u0026rsquo;s REPL sessions are not saved anywhere, and once the pairing sessions are over, Alice can\u0026rsquo;t easily replicate them.\nAlice has the sinking feeling that the system can only be understood at runtime, by manually working with the UI and observing the events that occur across the system. Alice doesn\u0026rsquo;t just have to load the whole system on her Mac, she has to load the whole thing in her brain.\nTests Alice intends to start by writing an acceptance test for her story. However, she can\u0026rsquo;t quite figure out where to put it.\nIs the billing service responsible? Or the case service? It seems to her like the service is responsible for the rules in her domain.\nBut the existing logic exists in the re-frame front end. There are a series of events that call various services already to see if a case can be closed. The case\u0026rsquo;s status comes from the \u0026ldquo;case\u0026rdquo; services. There are some calls to services named after different types of cases (\u0026ldquo;bankruptcy\u0026rdquo;, \u0026ldquo;acquisitions and mergers\u0026rdquo;, etc), plus another service called \u0026ldquo;tasks\u0026rdquo;.\nAlice asks Ben whether the logic should go in re-frame or in the \u0026ldquo;case\u0026rdquo; service. Ben tells her it it theoretically should go in the \u0026ldquo;case\u0026rdquo; service, but the easiest thing to do would be to put it in the frontend for now.\nHe tells her to use a subscription to look in the re-frame state, find the invoices, and see if any are unpaid. Then call the subscription in the view to disable the \u0026ldquo;Close Case\u0026rdquo; button.\nAs to tests, Ben says that there is not much \u0026ldquo;logic\u0026rdquo; because most of the code is calling other services (e.g., calling the billing service to find another invoice). All that information is already available in re-frame\u0026rsquo;s app-db. This is more IO than logic, Ben says. Let\u0026rsquo;s skip the tests.\nAlice is a little relieved. It\u0026rsquo;s her first ticket. The easier it is, the more quickly she can get it done. She has enough loaded in her brain to start to push some commits.\nShe\u0026rsquo;s a little distracted when another developer asks Ben about some regressions he had introduced refactoring the bankruptcy service. Ben insists that the code needed to be cleaned up. Yet Alice wonders: if it doesn\u0026rsquo;t work, is it really an improvement?\nShe pushes the thought out of her head, finishes her work, and opens her first pull request.\nBut as she closes her laptop for the day, she almost hears Rich Hickey\u0026rsquo;s voice whispering \u0026ldquo;it was easy, but was it simple?\u0026rdquo;\nLessons What are the main lessons we can draw?\nThe REPL is a double edged sword The REPL is a powerful tool. It enables us the visibility needed to solve complicated problems. It also enables us to tolerate creating complicated problems for ourselves.\nThe dynamism of the REPL is a double edged sword. By enabling us to work with systems at runtime, it allows us to build incredibly coupled systems that can only be understood at runtime.\nThe Distributed Monolith Distributed monoliths are systems that have all the downsides of microservices and monoliths without the benefits. They are characterized by tightly coupling and low cohesion.\nA common driver for the distributed monolith is the \u0026ldquo;smart\u0026rdquo; SPA, that knows too much about an application\u0026rsquo;s business logic and is therefore highly coupled to backend microservices.\nCohesive Business Logic Take the workflow of opening a case, moving the case through its lifecycle, and closing a case. There should be one place to go that expresses that workflow in this system.\nIn systems where the complexity lies in the business logic, cohesion is most urgent when it comes to the business logic.\nInstead of this, the Atticus Case Management system spreads the business logic between frontend and the backend services. In order to figure out who can take what action under what conditions in a workflow, the logic could be anywhere: in the view (e.g., a disabled button), in the reframe subscription, properties in re-frame\u0026rsquo;s app db, in an event handler, or it could be scattered throughout the back end services.\nSystems like this are incredibly brittle. They are business systems that poorly encode business rules.\nFrontends should be loosely coupled Reframe can have a high degree of efferent coupling. Sometimes its event handling system is used to coordinate sequences of commands to services.\nReframe app databases do not support information hiding well, and in a large application, there can be a great deal of shared state. Subscriptions can reach into any path in the app db.\nImplementations that are hard to test are of poor quality Especially in distributed systems. Clojure is no exception.\nThere are two difficulties with building tests. The first is not being skilled in writing good tests. Just like functional programming is a skill to be acquired, so too with testing.\nThe second difficulty, the more difficult one to surmount, is poor implementation. Bad implementations are hard to test.\nTDD Prevents Bad Implementations In order to easily testable a system must be:\nHighly cohesive business logic Loosely coupled Modular (i.e., pluggable) Thoughtful in terms of designing interfaces Structured in terms of the domain Built to expose measurement points And of course, these are the same properties that make robust systems.\nWriting the tests first makes poor implementation design very difficult.\nRemember that Alice could find no one place where that encoded the business logic for a particular workflow. If Ben\u0026rsquo;s team used test driven development, they would have started with high level tests expressing what the workflow does. This would have forced them to have a clear, unified place that expressed the business logic.\nAs a result, it would not be possible, or at least it would be extremely difficult, to build a system that has a low cohesion with the business logic.\nConsider the alternative: a team writes an implementation and then considers how to write a test for it. They come to the conclusion that the test is hard to write. (It\u0026rsquo;s a problem with testing itself, not our implementation.) It\u0026rsquo;s not even clear which repository the test belongs in!\nThey have discovered their implementation to be poorly designed. But rather than think critically about their design decisions, they are so committed to their prior thinking that they give up on testing at all!\nThis is how to write a legacy system.\nIs TDD a must? I don\u0026rsquo;t think the research is conclusive enough (yet) to say that test driven development is a professional duty of care. However, writing thorough tests is. Just as an attorney keeps a client\u0026rsquo;s confidence, and a doctor prescribes a certain standard of care, developers owe their employers and coworkers tests and testable systems.\nAnd test driven development makes it much easier to Design Systems to be testable, and write the test.\nWriting testable systems is mostly a culture change The benefits of writing tests are hard to grasp unless one i) already has a good understanding of software design, and ii) one has empathy for other developers.\nDevelopers who understand the importance of low coupling and high cohesion, who insist on distinguishing the \u0026ldquo;easy\u0026rdquo; from the \u0026ldquo;simple\u0026rdquo;, who think carefully about domains and their boundaries, tend to get the value proposition of tests.\nAppreciating tests means changing core beliefs about what makes good software.\nThere is no tradeoff between speed and quality One common canard in software development is the proclamation that there is a tradeoff one must between speed and quality, with the implication that tests slow you down.\nIn fact, empirical research has shown this to be false: speed and quality are positively correlated. See Forsgren, Humble, and Kim\u0026rsquo;s Accelerate. And writing tests is a driver of both.\nThe idea that there is a tradeoff comes from a blinkered view of the span between receiving a ticket and tossing it over the wall. Viewing the system as a whole (realizing the cost of defects, for example) and having empathy for other developers who must work with the code requires a broader perspective. It requires an understanding that the mental model you build up when writing the code is undocumented, not reflected in any executable specification.\nTaking a broader perspective, and having the rationality to submit our anecdotal opinions to the evidence is not easy. Like Cremononi refusing to look through the telescope, many will simply ignore evidence where it discredits their own opinion.\nWriting Legacy Clojure Michael Feathers defined legacy systems as those without tests. This is precisely the problem Alice faces: her new team\u0026rsquo;s code base is a legacy system from the jump.\nRighting the ship is going to require a dual approach. It requires two partitions. For new features, a test driven approach will go a long way toward a simpler, robust, understandable system. For the untested parts currently in place, the team will need to use strategies for managing legacy systems.\n","id":2,"section":"posts","summary":"This is the first in a multi-part series, \u0026ldquo;Brittle Clojure\u0026rdquo;. In this series, we will consider common patterns in Clojure which yield brittle systems, as well as methods to ensure robustness.\nNone of the basic principles for building robust software are unique. Most literature, however, is focused on object-oriented systems. Our point of view will sometimes zoom in to Clojure \u0026ldquo;in the small\u0026rdquo;, and sometimes zoom out to distributed systems built with Clojure.","tags":["clojure","architecture","microservices","brittle-clojure","robust-clojure"],"title":"Brittle Clojure: Creating Legacy Clojure Systems","uri":"https://thomascothran.tech/2023/07/brittle-clojure/","year":"2023"},{"content":"Among DORA\u0026rsquo;s more controversial findings is that trunk based development is superior to feature branching.\nTeams achieve higher levels of software delivery and operational performance (delivery speed, stability, and availability) if they follow these practices:\nHave three or fewer active branches in the application’s code repository. Merge branches to trunk at least once a day. Don’t have code freezes and don’t have integration phases. Since we know not to be Cremoninis, we won\u0026rsquo;t be distracted by whether trunk-based development meets the HN trendiness standard, and we\u0026rsquo;ll be skeptical of anectdotal appeals to experience\nStill, why does trunk based development predict higher performing teams?\nThe Core Principle of CI The core principle of Continuous Integration is that there should be \u0026ldquo;one interesting version\u0026rdquo;.\nWhen we have multiple versions, spread across feature branches, we end up with \u0026ldquo;merge hell\u0026rdquo;.\nMerge Hell Example Consider a scenario where two developers are working on separate features for the same application. They each create a feature branch and spend a week developing their respective features. At the end of the week, they\u0026rsquo;re ready to merge their changes back into the main branch, or the \u0026rsquo;trunk'.\nHowever, because they\u0026rsquo;ve been working in isolation, their changes conflict with each other, causing bugs that weren\u0026rsquo;t present when the features were tested on their individual branches. This is \u0026lsquo;merge hell\u0026rsquo;, and it can significantly slow down the development process and reduce the overall quality of the software.\nTrunk Based Development Trunk-based development is a strategy designed to avoid this problem. In this approach, all developers work on a single branch, the \u0026rsquo;trunk\u0026rsquo;.\nThey integrate their changes frequently, at least once a day, and these changes are immediately tested. This frequent integration and testing ensure that issues are caught and resolved early, reducing the risk of merge hell.\nThe most recent version of the trunk, assuming it has passed all tests, is considered the single \u0026ldquo;interesting\u0026rdquo; version of the application.\nMicroservices Microservices are a means of implementing a loosely coupled architecture. This too is predictive of higher performing software organizations.\nAs we transition from a monolithic architecture to a microservices environment, the principles of trunk-based development remain relevant, but they take on a new dimension.\nIn a microservices architecture, each service is developed, deployed, and scaled independently. This independence is a strength.\nHowever, the merge hell can re-emerge in even more painful form even if you practice trunk based development in your services.\nVersion Tetris Consider the common scenario of an enterprise application implemented with a single SPA and a set of supporting backend services. The SPA makes calls to these services, either directly or through an abstraction later like GraphQL.\nHowever, the changes to this service are not ready for end users yet. Either they haven\u0026rsquo;t been signed off by the QA team, or the end users need trainings before they can use the new features.\nThe reason doesn\u0026rsquo;t matter. What does? You start holding versions back.\nSuppose you have a dev, staging, UAT, and production environment. A feature is completed in development and deployed to staging for QAs to review. However, production users certainly aren\u0026rsquo;t ready yet, and UAT users don\u0026rsquo;t want disruptions.\nSo you decide not to deploy the more recent version to UAT or production.\nYou start to end up with a situation that looks like this:\nDev Version Staging Version UAT Version Prod Version Frontend SPA v100 v100 v90 v85 Auth Service v75 v75 v75 v75 Payments Service v50 v49 v45 v40 Snazzy Feature Service v60 v57 v53 v50 No two environments are the same! Inter-service dependencies are bad enough. But typically your front end will depend on most (or all) of the backend services. Just because your front end works in staging, doesn\u0026rsquo;t mean it will work in UAT or production.\nYou don\u0026rsquo;t have one interesting version of your system, you have four. And they\u0026rsquo;re probably interesting in the wrong way.\nBut wait! It doesn\u0026rsquo;t stop there \u0026ndash; the versions have dependencies on each other.\nThe frontend SPA will depend on various backend versions. For example, a change in payments on the frontend requires a change in the payments service on the backend.\nDistributed Merge Hell Now, you have the same problem as before, but much worse. We have recreated merge hell, and made it distributed.\nOur new \u0026ldquo;merge\u0026rdquo; is trying to identify the set of versions we can deliver to end users.\nOur new feature branches are the environments. The individual commits are the versions we bring into the environment.\nUsing versions to control features is feature branching on the service level.\nEscaping Merge Hell There\u0026rsquo;s only one way out. Versions should propagate to higher environments. You should have \u0026ldquo;one interesting version\u0026rdquo;.\nBut this brings us back to the need for continuous integration. To integrate is to bring something together into a whole. To do so continuously means to avoid batching the work.\nThe difficulty is this: to do this, you need a CI process that gives you confidence no regressions propagate to end users. That means an automated regression test suite you can depend on. It means contract testing to identify broken promises between services.\nBut the bad news is the good news. This is necessary anyway to delivery quality software. If the rush to continuous delivery forces improvements in the CI process, what are the down sides?\n","id":3,"section":"posts","summary":"Among DORA\u0026rsquo;s more controversial findings is that trunk based development is superior to feature branching.\nTeams achieve higher levels of software delivery and operational performance (delivery speed, stability, and availability) if they follow these practices:\nHave three or fewer active branches in the application’s code repository. Merge branches to trunk at least once a day. Don’t have code freezes and don’t have integration phases. Since we know not to be Cremoninis, we won\u0026rsquo;t be distracted by whether trunk-based development meets the HN trendiness standard, and we\u0026rsquo;ll be skeptical of anectdotal appeals to experience","tags":["DORA","empiricism","evidence-based-software","microservices","continuous-integration"],"title":"Distributed Merges and Continuous Integration","uri":"https://thomascothran.tech/2023/07/distributed-merges/","year":"2023"},{"content":"Differences of opinion about how we ought to write software have an air of the philosophical about them. Some prefer TDD and microservices, others may prefer monoliths and think that most testing is a waste of time. Or engineers may prefer to use continuous development methodologies, while businesses prefer a waterfall approach with decorative scrum ceremonies.\nAre we stuck with opinions? Must we be subjected to the obligatory \u0026ldquo;well, in my experience \u0026hellip;\u0026rdquo;? Are we simply expressing our personal feelings, or is there some truth to be had?\nFortunately, we have the tools to get past opinion and subjective experience. Just as we no longer hold that the world is flat, that the earth is the center of the universe, or that planets move about in crystalline spheres, so there are opinions in the context of software development that are no longer intellectually respectable to hold.\nAristotle and Galileo Our situation is much like early modern science.\nFor centuries, people thought that things fell faster in proportion to their weight. They thought the planets were unchangeable. For this they had a lifetime of anecdotal evidence and the authority of an expert: Aristotle.\nGalileo’s revolution lay in submitting the experience accumulated over millennia to rigorous quantitative measurement.Not everyone was convinced right away, Galileo recounts to Kepler that many Aristotelian philosophers refused even to look through his telescope. His colleague Cesare Cremonini allegedly even said:\nI do not wish to approve of claims about which I do not have any knowledge, and about things which I have not seen … and then to observe through those glasses gives me a headache. Enough! I do not want to hear anything more about this.\nThe lesson? Even with the evidence, one can only make progress if one is willing to abandon one’s own opinions in light of the evidence. Don’t be a Cremonini!\nEmpiricism \u0026gt; Intuition Is pure science is a special case? Does quantitative hypothesis testing does work in complex, concrete, human scenarios? Surely outside carefully controlled laboratories, our intuition and experience is more reliable than abstract, quantitative techniques.\nEvidence based medicine provides an instructive example. Its great struggle arose not so much from technological factors as human resistance. Doctors insisted that their intuition was more accurate in particular cases than the results of large studies.\nAnd we see why this is so psychologically difficult: what is the value of long years of experience if it can be set aside for a table of numbers?\nNow of course we are fortunate that when we need medical care that evidence-based medicine has been institutionalized as the standard of care\nNor is the medical case unique. Academics such as Paul Meehl and Philip Tetlock have studied for decades whether expert intuition is more accurate than very simple empirical statistical models in a variety of domains: psychology, geo-politics, economics, criminal justice, and other social sciences.\nWhich performs better? Intuitive judgments which rely on expert experience? Or quantitative empirical methods? The weight of the evidence is clear: quantitative-empirical methods are superior both for predicting general trends, and accurate judgments in particular cases.\nThis should not surprise us much. Consider how difficult it is to prioritize development projects without a financial analysis that predicts how each project affects profits and losses. Prioritization not based on hard numbers are inevitably based on how important something \u0026ldquo;feels\u0026rdquo;.\nAgain, consider how difficult estimation is when we try to use our intuition to determine how long a project might take. By comparison, statistical techniques like reference based forecasting or monte carlo simulations not only are faster, but more accurate.\nEmpirical Evidence for Software Development The Annual State of Devops report is the largest empirical study of the effectiveness of software development practices. Not only does it look at technical measurements (such as lead times); it considers economic effects such as profitability and market share.\nDora has gathered data from over 33,000 software developers over the last decade. The findings are summarized in a book called Accelerate.\nPerhaps the best way into the DORA findings is the new capabilities page. It sets out those practices which have empirical support.\nIn addition to technical capabilities (such as Cloud Infrastructure and Source Control), there are process and cultural capabilities.\nBeyond opinion Consider some common debates developers have. Most developers accept testing as part of being a software professional, yet there are many dissenters. Even those who favor testing often say that testing involves a tradeoff of speed for reliability.\nBut what does the evidence say? It may be unsurprising to learn that the conventional wisdeom is correct \u0026ndash; testing does improve reliability. What is more surprising is that there is no tradeoff between speed and reliability!\nThe effect is that the debates developers have have, at work or on Hacker News, around testing or continuous delivery, can be resolved. Everyone has opinions of course. The question is whether they are intelligent, informed opinions.\nAnd there are many things we can rule out as discredited (until more comprehensive evidence appears to the contrary). Here are a few of my favorites:\nTests don\u0026rsquo;t matter, or they slow you down, or developers don\u0026rsquo;t need to be involved in writing them Speed and reliability are opposed to one another It is safer to deploy weekly or monthly than daily or hourly Gitflow is better than trunk based development Batching features is better than releasing incrementally Once written (or pulled into a sprint), requirements should not change Microservices will slow you down Tight coupling is preferable to loose coupling Prior to the availability of empirical evidence, we are all Aristotelians, doing the best we can with our common sense experiences. But after the evidence is available, asserting one\u0026rsquo;s common sense is no more intelligent than Cremonini\u0026rsquo;s refusal to look through the telescope.\nEngineering and the Broader Organization Often, though, engineers are not debating among themselves, they are negotiating with business analysts, quality assurance departments, security teams, product owners, and managers.\nFor example, engineers are more liable to want test and deployment automation, greater autonomy and so on.\nDORA is useful here in showing that such suggestions are supported by the evidence, while the opposite practices (manual regression testing, manual and infrequent deployments) negatively affect a departments performance.\nThat is to say, the question should not be posed as \u0026ldquo;what engineers want\u0026rdquo; versus what \u0026ldquo;business analysts | QA | etc want\u0026rdquo;. The question is how can we avoid irrational decisions by considering the relevant evidence.\nUnfortunately, DORA points at why these conversations can be difficult. There are certain entrenched cultural factors which are difficult to dislodge. While few organizations are pathological, a generative, learning culture is not had by default. It is achieved, and only with significant effort.\nRational argument is not enough here. Certain psychological barriers must be surmounted. Evidence must be accompanied by rhetorical persuasion that speaks not only to what is correct, but to the feelings and emotions that may get in the way of objective judgment.\nAs a result, presenting the evidence and then directly identifying the cognitive biases that surface in response is perhaps not the most successful strategy. Sometimes it is better to just bring donuts.\nRecommended Reading Clark, Dominic A. \u0026ldquo;Human expertise, statistical models, and knowledge-based systems.\u0026rdquo; In Expertise and decision support, pp. 227-249. Boston, MA: Springer US, 1992. Dawes, Robyn M., David Faust, and Paul E. Meehl. \u0026ldquo;Clinical versus actuarial judgment.\u0026rdquo; Science 243, no. 4899 (1989): 1668-1674. Grove, William M., and Paul E. Meehl. \u0026ldquo;Comparative efficiency of informal (subjective, impressionistic) and formal (mechanical, algorithmic) prediction procedures: The clinical–statistical controversy.\u0026rdquo; Psychology, public policy, and law 2, no. 2 (1996): 293. Grove, William M., David H. Zald, Boyd S. Lebow, Beth E. Snitz, and Chad Nelson. \u0026ldquo;Clinical versus mechanical prediction: a meta-analysis.\u0026rdquo; Psychological assessment 12, no. 1 (2000): 19. Humble, Jez, and Gene Kim. Accelerate: The science of lean software and devops: Building and scaling high performing technology organizations. IT Revolution, 2018. ","id":4,"section":"posts","summary":"Differences of opinion about how we ought to write software have an air of the philosophical about them. Some prefer TDD and microservices, others may prefer monoliths and think that most testing is a waste of time. Or engineers may prefer to use continuous development methodologies, while businesses prefer a waterfall approach with decorative scrum ceremonies.\nAre we stuck with opinions? Must we be subjected to the obligatory \u0026ldquo;well, in my experience \u0026hellip;\u0026rdquo;?","tags":["DORA","empiricism","evidence-based-software"],"title":"Don't Be a Cremonini","uri":"https://thomascothran.tech/2023/07/dont-be-cremonini/","year":"2023"},{"content":"The polymath Blaise Pascal envisaged a triangle built of numbers. Pascal’s triangle — as it is usually called, despite the fact that its discovery predated Pascal by centuries — has the interesting property that each number is the sum of the two numbers directly above it.\nIn this post we will use Pascal’s triangle to demonstrate how recursion (i.e., a procedure that invokes itself in its definition) can be used to make complex problems easily soluble, using examples written in both Haskell and JavaScript. Wikipedia\nPascal’s triangle is constructed such that the first row has one number: 1. Each additional row adds one additional number. The third row, therefore, has three numbers; the 1,542nd row has 1,542 numbers. Each number is the sum of the two numbers above it, except for the 1 at the pinnacle.\nIf we want to solve a problem recursively, we start with what is easy. What is obvious about Pascal’s triangle? Well, to begin with, we have simply posited that the number in row 1 is 1. (This is the only number in the triangle that is not the sum of the numbers above it.) We also see that first and last column on each row will be a 1. The reason for this is that each of these is the sum of 1 and nothing (i.e, 0).\nLet’s say we need a function — let’s call it pt — that takes two integers, one representing a row and one the column. pt should return the number at that position. For instance, pt 1 1 should return 1, and pt 3 2 should return 3. (Note that we are not using zero based numbering for our rows and columns).\nOur type signature will be pt :: (Integer a) =\u0026gt; a -\u0026gt; a -\u0026gt; a. That is, is pt a function that takes two integers and returns an integer. Start with the easy answers\nWhen coming up with a recursive solution, we start with the easy answers. What’s the easiest case here? Well, the number at row 1 column 1 is 1!\npt :: Integer -\u0026gt; Integer -\u0026gt; Integer pt 1 1 = 1// JavaScript function pt(row, col) { if (a === 1 || b === 1) { return 1; } } That was easy. What else can we say for sure? For one thing, there is nothing at a column less than 1. Nothing = zero. So if we ask pt for a position where the column is less than 1, pt should return 0.\npt :: Integer -\u0026gt; Integer -\u0026gt; Integer pt row col | row == 1 \u0026amp;\u0026amp; col == 1 = 1 | col \u0026lt; 1 = 0// JavaScript function pt(row, col) { if (row === 1 \u0026amp;\u0026amp; col === 1) { return 1; } else if (col \u0026lt; 1) { return 0; } Come to think of it, if the column position is less than 1, that is just a special case where the number is outside Pascal’s triangle (in this case, to the left). What if the column position is outside the triangle to the right side? It would also be zero. We know that the total number of columns in a row is equal to the row number. Thus:\npt :: Integer -\u0026gt; Integer -\u0026gt; Integer pt row col | row == 1 \u0026amp;\u0026amp; col == 1 = 1 | col \u0026lt; 1 || col \u0026gt; row = 0// JavaScript function pt(row, col) { if (row === 1 \u0026amp;\u0026amp; col === 1) { return 1; } else if (col \u0026lt; 1 || col \u0026gt; row) { return 0; } } After the easy questions, then what?\nThe nice thing about recursion is that we can just state what we know, and the computer will just figure out what we don’t. We know that any column outside the triangle will equal zero, and we know the triangle’s pinnacle is one.\nWe don’t need to manually figure out the numbers for any other position; we can just ask the computer to figure it out for us. All we need to do is describe to the computer what we need it to find. We can treat the program as magic, or the computer as an oracle.\npt :: Integer -\u0026gt; Integer -\u0026gt; Integer pt row col | row == 1 \u0026amp;\u0026amp; col == 1 = 1 | col \u0026lt; 1 || col \u0026gt; row = 0 | otherwise = (pt (row - 1) (col - 1)) + (pt (row - 1) (col))// JavaScript function pt(row, col) { if (row === 1 \u0026amp;\u0026amp; col === 1) { return 1; } else if (col \u0026lt; 1 || col \u0026gt; row) { return 0; } else { return (pt (row - 1, col - 1)) + (pt (row - 1, col)); } } Magic!\nAnd there we have it! pt 1 1 returns 1, pt 3 2 returns two, and pt 17 5 returns 1820.\nBut it’s not really magic. There are a few things to keep in mind when constructing recursive functions, most importantly the base case imperative. Our base cases return answers, rather than recursively invoking the function. If we don’t hit the base cases eventually, the program will never terminate. The fact that the recursive call here decrements row each time means that we will eventually hit the first row.\nThe other thing to keep in mind is that this sort of recursive call is a form of tree recursion. The larger the numbers, the more resources the procedure will demand. The rate of growth here is exponential. My computer quickly gave me the answer to pt 17 5; I’m still waiting on the answer to pt 71 5.\n","id":5,"section":"posts","summary":"The polymath Blaise Pascal envisaged a triangle built of numbers. Pascal’s triangle — as it is usually called, despite the fact that its discovery predated Pascal by centuries — has the interesting property that each number is the sum of the two numbers directly above it.\nIn this post we will use Pascal’s triangle to demonstrate how recursion (i.e., a procedure that invokes itself in its definition) can be used to make complex problems easily soluble, using examples written in both Haskell and JavaScript.","tags":["recursion","haskell"],"title":"Haskell's Triangle","uri":"https://thomascothran.tech/2017/07/haskells-triangle/","year":"2017"},{"content":"A procedure is recursive if it invokes itself. Thus:\nconst toInfinityAndBeyond = num =\u0026gt; toInfinityAndBeyond(num + 1); Of course, toInfinityAndBeyond is only useful if, rather than seeking an answer, you want to blow your call stack. But you see the point: we find toInfinityAndBeyond in its own body. What possible use could this be?\nRecursion is often well suited to express the logic of a problem. Let’s take a simple problem: the conversion of Roman to Arabic numerals. One solution is this one:\nfunction deromanize (str) { var\tstr = str.toUpperCase(), validator = /^M*(?:D?C{0,3}|C[MD])(?:L?X{0,3}|X[CL])(?:V?I{0,3}|I[XV])$/, token = /[MDLV]|C[MD]?|X[CL]?|I[XV]?/g, key = {M:1000,CM:900,D:500,CD:400,C:100,XC:90,L:50,XL:40,X:10,IX:9,V:5,IV:4,I:1}, num = 0, m; if (!(str \u0026amp;\u0026amp; validator.test(str))) return false; while (m = token.exec(str)) num += key[m[0]]; return num; } We might be interested in solving this problem differently for a number of reasons, perhaps to avoid the pitfalls that go along with complex regular expressions, or because we want to avoid mutating variables. Most relevant for our purposes is the key object. Note how it includes not only M:1000, but also entries like CM: 900 and IV: 4. This hard wires crucial logic rather than expressing it.\nA First Stab Recursion gives us a more expressive way to approach the problem, one that allows us to use a single pure function that does not mutate variables. Here is a first stab:\nconst nums = {I: 1, V: 5, X: 10, L: 50, C: 100, M: 1000}; const arabify = (romNum) =\u0026gt; { if (romNum.length === 0) { return 0; } else if (nums[romNum[0]] \u0026lt; nums[romNum[1]]) { return (nums[romNum[1]] - nums[romNum[0]] + arabify(romNum.slice(2))); } else { return (nums[romNum[0]] + arabify(romNum.slice(1))); } } This has the benefit of expressing the logic of converting valid (or even some invalid!) Roman numerals to Arabic numerals, but it has some problems as well. Let’s consider the good before the bad. The Good\nIn the first place, we are not using variables or complex regular expressions. More to the point, in our nums object, we have no need of shortcuts like IX: 9 or CM: 900 — our program handles this for us. How?\nThe function arabify takes a string, which we are assuming to be valid Roman numerals. If that string is empty, it simply returns 0. An empty string is our base case, the point at which arabify stops calling itself. Without a base case, the arabify calls will just keep piling up on the stack until there is a stack overflow.\nIf the string passed to arabify is not empty, we must determine the relation of the first two characters. If the first character is less than the second character — nums[romNum[0]] \u0026lt; nums[romNum[1] — we know that the former should be subtracted from the latter. That is, if X represents 10 and C a 100, then XC is 90.\nWhy in the case where the first two characters require the first be subtracted from the second do we need the call again to arabify? Because we could have a case like XCII, where we not only need to subtract the first item from the second, but the result must be added to the remaining numerals.\nIf the first numeral is not less than the second numeral, the solution is straightforward: add the first numeral to the value of the rest. We can simply call arabify on the remainder of the string from Roman numerals to find that value.\nThe Bad One downside to arabify is that it’s not the most readable. Wouldn’t it be nice if instead of having to parse things like nums[romNum[0]] \u0026lt; nums[romNum[1], we could have nums[fst] \u0026lt; nums[snd] instead?\nThe second problem is unlikely to arise in this particular context, but pointing it out is useful. The calls to arabify are going to keep piling on the call stack until the base case is reached — an empty string, then each call can be evaluated and the final answer returned. This means that arabify will consume memory roughly proportionally with the size of the string passed in as an argument.\nFortunately, the ES6 spec now includes tail call optimization. For recursive calls in the tail position, the function calls need no longer pile up on the stack. Let’s take a second stab at arabify that makes this clearer.\nTail Call Optimization arabify2 uses tail calls and is a bit easier to read.\nconst nums = {I: 1, V: 5, X: 10, L: 50, C: 100, M: 1000}; const arabify2 = (romNum, sum=0) =\u0026gt; { const [fst, snd, rest] = [romNum[0], romNum[1], romNum.slice(2)]; if (!snd) {return nums[fst] ? nums[fst] + sum : sum;} else if (nums[snd] \u0026gt; nums[fst]) { return arabify2(rest, nums[snd] - nums[fst] + sum); } else {return arabify2(snd + rest, nums[fst] + sum);} } On line four, we use ES6 destructuring to so that we can say fst rather than romNum[0], snd rather than romNum[0], and rest rather than romNum.slice(2) or romNum.slice(1).\nNote the difference between the recursive calls to arabify2 in comparison to arabify: we don’t make a call to aribify2and then do something else with it. aribify2 uses the parameter sum to keep track of all the information it needs. There is no reason to ‘remember’ the previous aribify2 calls: the last call returns the result we are looking for.\n","id":6,"section":"posts","summary":"A procedure is recursive if it invokes itself. Thus:\nconst toInfinityAndBeyond = num =\u0026gt; toInfinityAndBeyond(num + 1); Of course, toInfinityAndBeyond is only useful if, rather than seeking an answer, you want to blow your call stack. But you see the point: we find toInfinityAndBeyond in its own body. What possible use could this be?\nRecursion is often well suited to express the logic of a problem. Let’s take a simple problem: the conversion of Roman to Arabic numerals.","tags":["recursion","javascript"],"title":"Recursion Made Simple with Roman Numerals","uri":"https://thomascothran.tech/2017/07/recursion-with-roman-numerals/","year":"2017"}],"tags":[{"title":"abstraction","uri":"https://thomascothran.tech/tags/abstraction/"},{"title":"architecture","uri":"https://thomascothran.tech/tags/architecture/"},{"title":"brittle-clojure","uri":"https://thomascothran.tech/tags/brittle-clojure/"},{"title":"clojure","uri":"https://thomascothran.tech/tags/clojure/"},{"title":"continuous-integration","uri":"https://thomascothran.tech/tags/continuous-integration/"},{"title":"design","uri":"https://thomascothran.tech/tags/design/"},{"title":"DORA","uri":"https://thomascothran.tech/tags/dora/"},{"title":"empiricism","uri":"https://thomascothran.tech/tags/empiricism/"},{"title":"evidence-based-software","uri":"https://thomascothran.tech/tags/evidence-based-software/"},{"title":"haskell","uri":"https://thomascothran.tech/tags/haskell/"},{"title":"javascript","uri":"https://thomascothran.tech/tags/javascript/"},{"title":"microservices","uri":"https://thomascothran.tech/tags/microservices/"},{"title":"recursion","uri":"https://thomascothran.tech/tags/recursion/"},{"title":"robust-clojure","uri":"https://thomascothran.tech/tags/robust-clojure/"}]}